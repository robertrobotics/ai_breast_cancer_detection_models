{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie środowiska wirtualnego i jego aktywacja (opcjonalne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m venv .env\n",
    "#!source .env/bin/activate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Źródło danych oraz potrzebne referencje\n",
    "\n",
    "> https://doi.org/10.1016/j.dib.2019.104863\n",
    "\n",
    "W poniższym przykładzie dane zostałe zapisane w tej samej lokalizacji, w której znajduje się skrypt w folderze 'dataset'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przed przystąpieniem do pisania kodu pamiętaj o zainstalowaniu potrzebnych paczek np. przez menedżer zależności PIP\n",
    "\n",
    "> pip install torch torchvision matplotlib tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budujemy klasę zarządzającą danymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Przypisujemy etykiety do klas\n",
    "class_to_label = {'normal': 0, 'benign': 1, 'malignant': 2}\n",
    "label_to_class = {0: 'normal', 1: 'benign', 2: 'malignant'}\n",
    "\n",
    "# Klasa definiująca kolekcję składającą się z: obrazu, maski i typu nowotworu\n",
    "# dziedziczy po klasie Dataset z modułu PyTorch\n",
    "class BUSIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, augment=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "\n",
    "        # Iteracja przez foldery \"normal\", \"benign\" i \"malignant\"\n",
    "        for class_name in ['normal', 'benign', 'malignant']:      # -> dla każdej klasy nowotworu:\n",
    "            class_dir = os.path.join(root_dir, class_name)        # -> tworzymy ścieżkę dla bieżącego typu w iteracji\n",
    "            label = class_to_label[class_name]                    # -> zapisujemy klasę nowotworu (label)\n",
    "                                                                  # -> i iterujemy się po wszystkich plikach, a\n",
    "            for file_name in os.listdir(class_dir):               # -> dla każdego pliku znalezionego w danym folderze:\n",
    "                if 'mask' not in file_name:                       # -> Ignorujemy maski przy wczytywaniu obrazów\n",
    "                    img_path = os.path.join(class_dir, file_name) # -> Zapisujemy ścieżkę do obrazu\n",
    "                    mask_path = os.path.join(class_dir, file_name.replace(\".png\", \"_mask.png\")) # -> Zapisujemy ścieżkę do maski\n",
    "                    \n",
    "                    # Dodajemy tylko istniejące maski\n",
    "                    if os.path.exists(mask_path):\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.mask_paths.append(mask_path)\n",
    "                        self.labels.append(label)\n",
    "\n",
    "    def __len__(self): # własna definicja długości kolekcji jako ilość plików obrazów\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx): # własna definicja pobrania obiektu z kolekcji\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
    "\n",
    "        # Augmentacja\n",
    "        if self.augment:\n",
    "            image, mask = self.augment(image, mask)\n",
    "            \n",
    "        # Transformacja\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Zwracamy obraz, maskę i etykietę klasy\n",
    "        return image, mask, self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definujemy klasę opisującą architekturę sieci UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(UNet, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Definiowanie bloku konwolucyjnego\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        # Definiowanie bloku klasyfikacyjnego\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # Pooling uśredniający\n",
    "            nn.Flatten(),             # Spłaszczenie obrazu wyjściowego z warstwy centralnej sieci UNet\n",
    "            nn.Linear(512, 1024),     # Standardowa warstwa liniowa\n",
    "            nn.ReLU(),                # Funkcja aktywacji ReLU\n",
    "            nn.Dropout(0.4),          # Dropout\n",
    "            nn.Linear(1024, 3)         # Na wyjściu 3 klasy po klasyfikacji\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        # Bloki UNet\n",
    "        self.encoder1 = conv_block(1, 32).to(device=self.device).to(torch.float32)\n",
    "        self.encoder2 = conv_block(32, 64).to(device=self.device).to(torch.float32)\n",
    "        self.encoder3 = conv_block(64, 128).to(device=self.device).to(torch.float32)\n",
    "        self.encoder4 = conv_block(128, 256).to(device=self.device).to(torch.float32)\n",
    "        \n",
    "        self.center = conv_block(256, 512).to(device=self.device).to(torch.float32)\n",
    "        \n",
    "        self.decoder4 = conv_block(256 + 256, 256).to(device=self.device).to(torch.float32)\n",
    "        self.decoder3 = conv_block(128 + 128, 128).to(device=self.device).to(torch.float32)\n",
    "        self.decoder2 = conv_block(64 + 64, 64).to(device=self.device).to(torch.float32)\n",
    "        self.decoder1 = conv_block(32 + 32, 32).to(device=self.device).to(torch.float32)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2).to(device=self.device).to(torch.float32)\n",
    "        self.final_conv = nn.Conv2d(32, 1, kernel_size=1).to(device=self.device).to(torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e1 = self.dropout(e1)\n",
    "        e1_pooled = self.pool(e1)\n",
    "\n",
    "        e2 = self.encoder2(e1_pooled)\n",
    "        e2 = self.dropout(e2)\n",
    "        e2_pooled = self.pool(e2)\n",
    "        \n",
    "        e3 = self.encoder3(e2_pooled)\n",
    "        e3 = self.dropout(e3)\n",
    "        e3_pooled = self.pool(e3)\n",
    "        \n",
    "        e4 = self.encoder4(e3_pooled)\n",
    "        e4 = self.dropout(e4)\n",
    "        e4_pooled = self.pool(e4)\n",
    "\n",
    "        center_bridge_output = self.center(e4_pooled)\n",
    "\n",
    "        center_bridge_upsampler = nn.ConvTranspose2d(center_bridge_output.shape[1], center_bridge_output.shape[1] // 2, kernel_size=2, stride=2).to(device=self.device).to(torch.float32)\n",
    "        center_bridge_unsampled = center_bridge_upsampler(center_bridge_output)\n",
    "\n",
    "        # Dopasowanie rozmiaru warstwy dekodera do warstwy enkodera, aby podczas zaokrąglania nie doszło do pomyłki\n",
    "        center_bridge_sized = F.interpolate(center_bridge_unsampled, size=(e4.size(2), e4.size(3)), mode='bilinear', align_corners=False)\n",
    "        d4_input = torch.cat([center_bridge_sized, e4], dim=1).to(device=self.device).to(torch.float32)\n",
    "        d4 = self.decoder4(d4_input)\n",
    "\n",
    "        d4_upsampler = nn.ConvTranspose2d(d4.shape[1], d4.shape[1] // 2, kernel_size=2, stride=2).to(device=self.device).to(torch.float32)\n",
    "        upsampled_d4 = d4_upsampler(d4)\n",
    "        # Dopasowanie rozmiaru warstwy dekodera do warstwy enkodera, aby podczas zaokrąglania nie doszło do pomyłki\n",
    "        upsampled_d4 = F.interpolate(upsampled_d4, size=(e3.size(2), e3.size(3)), mode='bilinear', align_corners=False)\n",
    "        d3_input = torch.cat([upsampled_d4, e3], dim=1)\n",
    "        d3 = self.decoder3(d3_input)\n",
    "\n",
    "        d3_upsampler = nn.ConvTranspose2d(d3.shape[1], d3.shape[1] // 2, kernel_size=2, stride=2).to(device=self.device).to(torch.float32)\n",
    "        upsampled_d3 = d3_upsampler(d3)\n",
    "        # Dopasowanie rozmiaru warstwy dekodera do warstwy enkodera, aby podczas zaokrąglania nie doszło do pomyłki\n",
    "        upsampled_d3 = F.interpolate(upsampled_d3, size=(e2.size(2), e2.size(3)), mode='bilinear', align_corners=False)\n",
    "        d2_input = torch.cat([upsampled_d3, e2], dim=1)\n",
    "        d2 = self.decoder2(d2_input)\n",
    "\n",
    "        d2_upsampler = nn.ConvTranspose2d(d2.shape[1], d2.shape[1] // 2, kernel_size=2, stride=2).to(device=self.device).to(torch.float32)\n",
    "        upsampled_d2 = d2_upsampler(d2)\n",
    "        # Dopasowanie rozmiaru warstwy dekodera do warstwy enkodera, aby podczas zaokrąglania nie doszło do pomyłki\n",
    "        upsampled_d2 = F.interpolate(upsampled_d2, size=(e1.size(2), e1.size(3)), mode='bilinear', align_corners=False)\n",
    "        d1_input = torch.cat([upsampled_d2, e1], dim=1)\n",
    "        d1 = self.decoder1(d1_input)\n",
    "        \n",
    "        segmentation_output = self.final_conv(d1)\n",
    "        classification_output = self.classifier(center_bridge_output)\n",
    "\n",
    "        return segmentation_output, classification_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja pomocniczna do prezentacji danych podczas treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funkcja do wyświetlania obrazów\n",
    "def visualize_predictions(model, data_loader, device, epoch):\n",
    "    model.eval()  # Przełączamy model w tryb ewaluacji\n",
    "    with torch.no_grad():\n",
    "        for images, masks, labels in data_loader:\n",
    "            # Pobierz jeden batch z loadera\n",
    "            images, masks = images.to(device).to(torch.float32), masks.to(device).to(torch.float32)\n",
    "            labels = labels.to(device).to(torch.float32)\n",
    "            \n",
    "            # Przewidywanie maski przez model\n",
    "            predicted_masks, predicted_labels = model(images)\n",
    "            \n",
    "            # Konwersja tensora na NumPy dla wizualizacji\n",
    "            input_image = images[0].cpu().permute(1, 2, 0).numpy()  # Przekształcamy na HWC\n",
    "            true_mask = masks[0].cpu().squeeze().numpy()  # Maska referencyjna\n",
    "            pred_mask = predicted_masks[0].cpu().squeeze().numpy()  # Maska przewidywana skopiowana do CPU\n",
    "\n",
    "            true_label = label_to_class[int(labels[0].cpu().item())]\n",
    "            # klasyfikator zwraca prawdopodobieństwo dla każdej klasy, wybieramy tę o największym (torch.argmax)\n",
    "            pred_label = label_to_class[torch.argmax(predicted_labels[0]).item()]\n",
    "\n",
    "            # Progowanie do wartości binarnych (np. próg 0.5)\n",
    "            # pred_binary_mask = (pred_mask > 0.5).astype(np.float32)\n",
    "            \n",
    "            # Tworzenie wykresów\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(input_image, cmap=\"gray\")\n",
    "            plt.title(f\"Oryginalny obraz\")\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(true_mask, cmap=\"gray\")\n",
    "            plt.title(f\"Maska referencyjna  (klasa: {true_label})\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(pred_mask, cmap='gray')\n",
    "            plt.colorbar()\n",
    "            plt.title(f\"Przewidywana maska (klasa: {pred_label})\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Wyświetlanie wyników\n",
    "            plt.suptitle(f\"Epoka: {epoch}\")\n",
    "            plt.show()\n",
    "            \n",
    "            break  # Wyświetlamy tylko pierwszy batch z każdej epoki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja licząca błąd dla obu problemów: segmentacji oraz klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcje straty\n",
    "segmentation_loss_fn = nn.BCEWithLogitsLoss()   # Dla segmentacji BCEWithLogitsLoss (BCELoss + funkcja sigmoidalna wewnątrz dla logitów)\n",
    "classification_loss_fn = nn.CrossEntropyLoss()  # Dla klasyfikacji wielu klas standardowo CrossEntropy\n",
    "\n",
    "def combined_loss(segmentation_output, classification_output, target_mask, target_class):\n",
    "    # Obliczenie strat\n",
    "    segmentation_loss = segmentation_loss_fn(segmentation_output, target_mask)\n",
    "    classification_loss = classification_loss_fn(classification_output, target_class)\n",
    "    \n",
    "    total_loss = segmentation_loss + classification_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja treningu i walidacji (inferencji) modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model = model.to(device).float()\n",
    "    \n",
    "    # Inicjalizacja paska postępu\n",
    "    progress_bar = tqdm(data_loader, desc=\"Trening\", leave=True, position=0)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Wczytaj batch\n",
    "        images, masks, labels = batch\n",
    "        images = images.to(device).to(torch.float32)\n",
    "        masks = masks.to(device).to(torch.float32)\n",
    "        labels = labels.to(device).to(torch.float32)\n",
    "\n",
    "        # Resetuj gradienty\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        segmentation_output, classification_output = model(images)\n",
    "        \n",
    "        # Oblicz stratę\n",
    "        loss = combined_loss(segmentation_output, classification_output, masks, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass i optymalizacja\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Aktualizuj pasek postępu\n",
    "        progress_bar.set_postfix({\"Strata\": loss.item()})\n",
    "    \n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Inicjalizacja paska postępu\n",
    "    progress_bar = tqdm(data_loader, desc=\"Walidacja\", leave=True, position=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            images, masks, labels = batch\n",
    "            images, masks, labels = images.to(device), masks.to(device), labels.to(device)\n",
    "        \n",
    "            segmentation_output, classification_output = model(images)\n",
    "            loss = combined_loss(segmentation_output, classification_output, masks, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Aktualizuj pasek postępu\n",
    "            progress_bar.set_postfix({\"Strata\": loss.item()})\n",
    "    \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasa pomocnicza do augmentacji obrazów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Augmentacje dla obrazów i masek\n",
    "class Augmentation:\n",
    "    def __init__(self, rotation=15, scale=(0.9, 1.1), flip_prob=0.5):\n",
    "        self.rotation = rotation\n",
    "        self.scale = scale\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # Losowy obrót\n",
    "        angle = random.uniform(-self.rotation, self.rotation)\n",
    "        image = F.rotate(image, angle)\n",
    "        mask = F.rotate(mask, angle)\n",
    "\n",
    "        # Losowe skalowanie\n",
    "        scale_factor = random.uniform(self.scale[0], self.scale[1])\n",
    "        new_size = [int(image.size[1] * scale_factor), int(image.size[0] * scale_factor)]\n",
    "        image = F.resize(image, new_size)\n",
    "        mask = F.resize(mask, new_size)\n",
    "\n",
    "        # Losowe odbicie poziome\n",
    "        if random.random() < self.flip_prob:\n",
    "            image = F.hflip(image)\n",
    "            mask = F.hflip(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje do zapisu i załadowania naszego modelu w przypadku długiego treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, episode, filename=\"model_checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'episode': episode,  # Numer epoki, aby można było wznowić trening\n",
    "        'model_state_dict': model.state_dict(),  # Wagi modelu\n",
    "        'optimizer_state_dict': optimizer.state_dict()  # Parametry optymalizatora\n",
    "    }\n",
    "    torch.save(checkpoint, filename)  # Zapisujemy do pliku\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # Załaduj wagi modelu\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # Załaduj stan optymalizatora\n",
    "    episode = checkpoint['episode']  # Załaduj numer epoki\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skrypt treningu i walidacji modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wybrany hardware obliczeniowy: MPS\n",
      "Epoka 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trening: 100%|██████████| 156/156 [01:39<00:00,  1.57it/s, Strata=1.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia strata treningowa: 1.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Walidacja: 100%|██████████| 39/39 [00:03<00:00, 12.26it/s, Strata=1.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia strata walidacyjna: 1.2811\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŚrednia strata walidacyjna: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Wyświetlenie przykładowych predykcji po każdej epoce\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mvisualize_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Zapisujemy nasz trenowany model co 10-tą epokę\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[140], line 22\u001b[0m, in \u001b[0;36mvisualize_predictions\u001b[0;34m(model, data_loader, device, epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m true_label \u001b[38;5;241m=\u001b[39m label_to_class[\u001b[38;5;28mint\u001b[39m(labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# klasyfikator zwraca prawdopodobieństwo dla każdej klasy, wybieramy tę o największym (torch.argmax)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_to_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Progowanie do wartości binarnych (np. próg 0.5)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# pred_binary_mask = (pred_mask > 0.5).astype(np.float32)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Tworzenie wykresów\u001b[39;00m\n\u001b[1;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Scieżka folderu danych USG\n",
    "data_dir = 'dataset'\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'gpu'\n",
    "\n",
    "print(f\"Wybrany hardware obliczeniowy: {str.upper(device)}\")\n",
    "\n",
    "# Augmentacja, jeśli chcemy stosować \n",
    "augmentation = Augmentation(rotation=15, scale=(0.9, 1.1), flip_prob=0.5)\n",
    "\n",
    "# Przekształcenie obrazów\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((279, 233)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "# Ładowanie zbioru danych\n",
    "dataset = BUSIDataset(root_dir=data_dir, transform=transform, augment=None)\n",
    "\n",
    "# Podział na trening i walidację\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=22)\n",
    "\n",
    "# Obiekty DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=False)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'eval': val_loader}\n",
    "\n",
    "# Definicja modelu UNet\n",
    "model = UNet(device=device)\n",
    "\n",
    "# Optymalizator\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Trening\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoka {epoch + 1}/100\")\n",
    "    \n",
    "    # Trening\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"Średnia strata treningowa: {train_loss:.4f}\")\n",
    "    \n",
    "    # Walidacja\n",
    "    val_loss = validate_one_epoch(model, val_loader, device)\n",
    "    print(f\"Średnia strata walidacyjna: {val_loss:.4f}\")\n",
    "    \n",
    "    # Wyświetlenie przykładowych predykcji po każdej epoce\n",
    "    visualize_predictions(model, val_loader, device, epoch)\n",
    "\n",
    "    trained_model = model\n",
    "\n",
    "    # Zapisujemy nasz trenowany model co 10-tą epokę\n",
    "    if epoch % 10 == 0:\n",
    "        save_checkpoint(trained_model, optimizer, \n",
    "                        epoch, filename=f\"model_checkpoint_{epoch}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
